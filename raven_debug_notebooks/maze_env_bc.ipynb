{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaxrl_m.envs\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 14:10:58.881252: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-18 14:10:58.907674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-18 14:10:59.366567: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/max/anaconda3/envs/offline/lib/python3.8/site-packages/transforms3d/quaternions.py:26: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  _MAX_FLOAT = np.maximum_sctype(np.float)\n",
      "pybullet build time: Nov 28 2023 23:51:11\n",
      "/home/max/anaconda3/envs/offline/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:48: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  min_version = LooseVersion(MIN_TF_VERSION)\n",
      "/home/max/anaconda3/envs/offline/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:54: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.13.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce RTX 3090 Ti/PCIe/SSE2\n",
      "GL_VERSION=3.3.0 NVIDIA 525.105.17\n",
      "GL_SHADING_LANGUAGE_VERSION=3.30 NVIDIA via Cg compiler\n",
      "pthread_getconcurrency()=0\n",
      "Version = 3.3.0 NVIDIA 525.105.17\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce RTX 3090 Ti/PCIe/SSE2\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "text argument:dependencies/ravens/ravens/environments/assets\n",
      "int args: ["
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ven = NVIDIA Corporation\n",
      "> \u001b[0;32m/home/max/Distributional-Preference-Learning/PreferenceTransformer/d4rl/d4rl/offline_env.py\u001b[0m(95)\u001b[0;36mget_dataset\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     93 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     94 \u001b[0;31m        \u001b[0;31m# Run a few quick sanity checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 95 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'observations'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'actions'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rewards'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'terminals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     96 \u001b[0;31m            \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dataset is missing key %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     97 \u001b[0;31m        \u001b[0mN_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'observations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"sort-easy-v0\", disp=True, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# np array with shape (frames, height, width, channels)\n",
    "def create_video(ims, actions):\n",
    "    video = np.array(ims)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    im = plt.imshow(video[0,:,:,:])\n",
    "\n",
    "    plt.close() # this is required to not display the generated image\n",
    "\n",
    "    def init():\n",
    "        im.set_data(video[0,:,:,:])\n",
    "\n",
    "    def animate(i):\n",
    "        im.set_data(video[i,:,:,:])\n",
    "        im.axes.set_title(f\"Action {i}: {actions[i]}\")\n",
    "        return im\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0],\n",
    "                                interval=500)\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxrl_m.learners.d4rl_utils import new_get_trj_idx\n",
    "dataset = env.get_dataset()\n",
    "print(dataset[\"observations\"].shape)\n",
    "dataset['dones_float'] = dataset[\"terminals\"].astype(np.float32)\n",
    "trj_idx = new_get_trj_idx(dataset)\n",
    "\n",
    "def get_traj(i):\n",
    "    # if len(dataset[\"observations\"].shape) == 2:\n",
    "    #     traj = {\n",
    "    #         \"observations\": dataset[\"observations\"],\n",
    "    #         \"actions\": dataset[\"actions\"],\n",
    "    #         \"rewards\": dataset[\"rewards\"],\n",
    "    #         \"dones\": dataset[\"dones_float\"],    \n",
    "    #     }\n",
    "    # else:\n",
    "    traj = {\n",
    "        \"observations\": dataset[\"observations\"][trj_idx[i][0]:trj_idx[i][1]+1],\n",
    "        \"actions\": dataset[\"actions\"][trj_idx[i][0]:trj_idx[i][1]+1],\n",
    "        \"rewards\": dataset[\"rewards\"][trj_idx[i][0]:trj_idx[i][1]+1],\n",
    "        \"dones\": dataset[\"dones_float\"][trj_idx[i][0]:trj_idx[i][1]+1],\n",
    "    }\n",
    "    return traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "\n",
    "def vis_trj(i):\n",
    "    traj = get_traj(i)\n",
    "    return visualise_trajectory(env, traj)\n",
    "\n",
    "def reset_scene(env, traj):\n",
    "    if traj[\"observations\"][0].shape[0] == 19:\n",
    "        init_obs = traj[\"observations\"][0][7:]\n",
    "    else:\n",
    "        init_obs = traj[\"observations\"][0]\n",
    "    init_pose_obs = []\n",
    "    for i in range(4):\n",
    "        o = init_obs[i*3: (i+1)*3]\n",
    "        init_pose_obs.append(o[:3])\n",
    "    env.unwrapped.env.task.init_pose(init_pose_obs)\n",
    "    # env.reset_end_effector(traj[\"observations\"][0, :7])\n",
    "    env.reset()\n",
    "    return env.get_obs()\n",
    "\n",
    "def visualise_trajectory(env, traj=None, model=None, use_seq=False):\n",
    "    # print(model)\n",
    "    assert traj or model\n",
    "    if traj is None:\n",
    "        obs = env.reset()\n",
    "        MAX_LEN = 100\n",
    "    else:\n",
    "        obs = reset_scene(env, traj)\n",
    "        MAX_LEN = len(traj[\"observations\"])\n",
    "\n",
    "    done = False\n",
    "    t = 0\n",
    "    ims = []\n",
    "    actions = []\n",
    "    o_a = {\n",
    "        \"observations\": [],\n",
    "        \"actions\": [],\n",
    "        \"errs\": []\n",
    "    }\n",
    "    \n",
    "    while not done and t < MAX_LEN:\n",
    "        if use_seq:\n",
    "            obs = np.ones(obs.shape)*t\n",
    "        if model:\n",
    "            # print(\"Using model\")\n",
    "            # obs_ = torch.tensor(traj[\"observations\"][t], dtype=torch.float32)\n",
    "            action = model(torch.tensor(obs, dtype=torch.float32)).detach().numpy()\n",
    "            # print(\"suction cup\", action[-1])\n",
    "            \n",
    "            # err = np.linalg.norm(action - traj[\"actions\"][t])\n",
    "        else:\n",
    "            action = traj[\"actions\"][t]\n",
    "            err = 0.0\n",
    "        o_a[\"observations\"].append(obs)\n",
    "        o_a[\"actions\"].append(action)\n",
    "        # o_a[\"errs\"].append(err)\n",
    "        obs, _, done, _ = env.step(action)\n",
    "        actions.append(action)\n",
    "        t+=1\n",
    "        ims.append(env.render(mode=\"rgb_array\"))\n",
    "    env.unwrapped.env.step(None)\n",
    "    o_a[\"observations\"] = np.array(o_a[\"observations\"])\n",
    "    o_a[\"actions\"] = np.array(o_a[\"actions\"])\n",
    "    # o_a[\"errs\"] = np.array(o_a[\"errs\"])\n",
    "    return create_video(ims, actions), o_a\n",
    "\n",
    "oracle_trajectory = get_traj(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = []\n",
    "for t in oracle_trajectory[\"actions\"]:\n",
    "    z.append(t[-1])\n",
    "\n",
    "plt.plot(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, observations, actions):\n",
    "        self.observations = observations\n",
    "        self.actions = actions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        observation = self.observations[idx]\n",
    "        action = self.actions[idx]\n",
    "\n",
    "        return observation, action\n",
    "\n",
    "# import wandb\n",
    "def get_model(use_seq=False, traj=None, N=1000, num_samples=None):\n",
    "    # wandb.init(project=\"ravens bc\", config={\"use_seq\": use_seq, \"num_samples\": num_samples, \"traj\": traj is not None, \"N\": N})\n",
    "    if traj is None:    \n",
    "        actions = dataset[\"actions\"]\n",
    "        observations = dataset[\"observations\"]\n",
    "        if num_samples:\n",
    "            observations = observations[:num_samples]\n",
    "            actions = actions[:num_samples]\n",
    "            train_observations, eval_observations, train_actions, eval_actions = observations, observations, actions, actions\n",
    "        else:\n",
    "        # # Split the dataset into train and eval sets\n",
    "            train_observations, eval_observations, train_actions, eval_actions = train_test_split(observations, actions, test_size=0.2, random_state=42)\n",
    "    else:\n",
    "        len_traj = len(traj[\"observations\"])\n",
    "        if use_seq:\n",
    "            observations = np.array([np.ones_like(traj['observations'][i])*i for i in range(len_traj)], dtype=np.float32)\n",
    "        else:\n",
    "            observations = np.array([traj['observations'][i] for i in range(len(traj))], dtype=np.float32)\n",
    "        actions = np.array([traj['actions'][i] for i in range(len_traj)], dtype=np.float32)\n",
    "        train_observations, eval_observations, train_actions, eval_actions = observations, observations, actions, actions\n",
    "\n",
    "    # Create train and eval datasets\n",
    "    train_dataset = CustomDataset(train_observations, train_actions)\n",
    "    eval_dataset = CustomDataset(eval_observations, eval_actions)\n",
    "\n",
    "    # Create dataloaders for train and eval datasets\n",
    "    batch_size = 256\n",
    "    print(len(train_dataset), len(eval_dataset), \"dataset lengths\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Create a 2-layer MLP for supervised learning\n",
    "    class MLP(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(MLP, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(hidden_size, output_size),\n",
    "            )\n",
    "            self.activation = torch.nn.Tanh()\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.model(x)\n",
    "            return self.activation(x)\n",
    "            return x\n",
    "\n",
    "    # Define the input size, hidden size, and output size for the MLP\n",
    "    input_size = len(observations[0])\n",
    "    hidden_size = 256\n",
    "    output_size = len(actions[0])\n",
    "\n",
    "    # Create an instance of the MLP\n",
    "    mlp = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "    # Print the MLP architecture\n",
    "    # print(mlp)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = N\n",
    "    print_every = N//10\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    prev_loss = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = mlp(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the training loss\n",
    "            train_loss += loss.item()\n",
    "            # print(\"inputs\", inputs)\n",
    "            # print(\"targets\", targets)\n",
    "            # print(\"outputs\", outputs)\n",
    "            # print(\"\\n\\n\")\n",
    "\n",
    "        # Compute the average training loss for the epoch\n",
    "        train_loss /= len(train_dataloader)\n",
    "\n",
    "        # Evaluate the model on the eval dataset\n",
    "        eval_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in eval_dataloader:\n",
    "                outputs = mlp(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "        # Compute the average eval loss for the epoch\n",
    "        eval_loss /= len(eval_dataloader)\n",
    "        eval_losses.append(eval_loss)\n",
    "        # wandb.log({\"train_loss\": train_loss, \"eval_loss\": eval_loss})\n",
    "        if eval_loss < prev_loss:\n",
    "            prev_loss = eval_loss\n",
    "            # torch.save(mlp, \"best_model.pt\")\n",
    "\n",
    "        # Print the training and eval loss for the epoch\n",
    "        if (epoch+1)%100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss}, Eval Loss: {eval_loss}\")\n",
    "    return mlp, observations, actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORACLE_TRAJ\n",
    "anim, traj_or = visualise_trajectory(env, oracle_trajectory)\n",
    "print(traj_or[\"errs\"])\n",
    "# HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEQ_TRAJ\n",
    "mlp_seq, observations, actions  = get_model(use_seq=True, traj=oracle_trajectory, N=10)\n",
    "anim, traj_seq = visualise_trajectory(env, oracle_trajectory, mlp_seq, use_seq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BC_TRAJ\n",
    "#mlp_bc, observations, actions  = get_model(False, oracle_trajectory, N=10)\n",
    "\n",
    "anim, traj_bc = visualise_trajectory(env, oracle_trajectory, mlp_bc, False)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FULL BC\n",
    "mlp_bc, observations, actions  = get_model(False, N=1)\n",
    "mlp_bc.load_state_dict(torch.load(\"bc_model.pt\"))\n",
    "for _ in range(10):\n",
    "    env.set_mode(1)\n",
    "    try:\n",
    "        anim, traj_bc = visualise_trajectory(env, model=mlp_bc)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "# HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_bc[\"errs\"]\n",
    "traj = get_traj(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_bc[\"observations\"][0] - traj[\"observations\"][0][7:], traj_bc[\"actions\"][0] - traj[\"actions\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_bc[\"observations\"][1] - traj[\"observations\"][1][7:], traj_bc[\"actions\"][1] - traj[\"actions\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_actions = mlp_bc(torch.tensor(traj[\"observations\"][1][7:], dtype=torch.float32)).detach().numpy()\n",
    "model_actions_bc = mlp_bc(torch.tensor(traj_bc[\"observations\"][1], dtype=torch.float32)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_bc[\"observations\"][1] - traj[\"observations\"][1][7:], model_actions - model_actions_bc, model_actions - traj[\"actions\"][1], model_actions_bc - traj[\"actions\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mlp_bc.state_dict(), \"bc_model.pt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mget_dataset()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.get_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "offline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
